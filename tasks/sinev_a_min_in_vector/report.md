# Минимальное значение элементов вектора

- Студент: Синев Артём Александрович, группа 3823Б1ПР2
- Технологии: SEQ, MPI
- Вариант: 4

## 1. Введение

Задача поиска минимального элемента в векторе является одной из фундаментальных операций в вычислительной математике и анализе данных. Данная работа посвящена разработке и реализации параллельного алгоритма для поиска минимального элемента в векторе с использованием технологии MPI (Message Passing Interface). Алгоритм обеспечивает эффективное распределение вычислений между процессами и позволяет масштабировать решение на большое количество вычислительных узлов.

## 2. Постановка задачи

Дан вектор целых чисел. Требуется найти минимальный элемент.

**Входные данные:**
- vector - одномерный вектор целых чисел произвольной длины N

**Выходные данные:**
- min_value - целое число, содержащее минимальное значение в векторе

**Ограничения:**
- Вектор должен содержать хотя бы один элемент
- Элементы вектора - целые числа (int)
- Допускаются отрицательные значения

**Пример:**

Входные данные: vector = [5, 3, 8, 1, 9, 2]
Выходные данные: min_value = 1

## 3. Описание базового алгоритма (последовательная версия)

Последовательный алгоритм поиска минимального элемента в векторе выполняет линейный проход по всем элементам с обновлением текущего минимального значения.

Алгоритм состоит из следующих шагов:

1. **Инициализация**: Установка начального минимального значения как первого элемента вектора
2. **Обработка строк**: Для каждого элемента от 1 до N-1::
   - Сравнение текущего элемента с сохраненным минимальным значением
   - Обновление минимального значения при обнаружении меньшего элемента
3. **Завершение**: Возврат найденного минимального значения

**Сложность алгоритма**

**Время:** O(N)
- Один проход по всем элементам вектора
- N-1 операций сравнения

**Память:** O(N)
- O(N) для хранения входного вектора
- O(1) для временных переменных
- **Итого:** O(N)

## 4. Схема распараллеливания

Параллельная реализация основана на делении матрицы по строкам между доступными процессами.
Матрица разделяется на блоки строк с использованием стратегии блочного циклического распределения:

```cpp
base_rows = rows / proc_count      // Базовое количество строк на процесс
extra_rows = rows % proc_count     // Остаточные строки для распределения

// Процессы с rank < extra_rows получают дополнительную строку
rows_per_process[rank] = base_rows + (rank < extra_rows ? 1 : 0)
```

**Роли процессов**
- **rank 0:** Координирует выполнение, распределяет данные, собирает результаты
- **rank 1..P-1:** Выполняют вычисления на выделенных порциях данных

**Схема коммуникации**
- **Фаза распределения:** Ведущий процесс знает всю матрицу
- **Фаза вычислений:** Каждый процесс независимо вычисляет максимумы для своих строк
- **Фаза сбора:** Остальные процессы отправляют результаты ведущему
- **Фаза синхронизации:** Ведущий процесс рассылает финальный результат всем процессам

## 5. Детали реализации

**Файлы:**
- `common/include/common.hpp` - определение типов данных
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` - последовательная реализация
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` - параллельная реализация
- `tests/functional/main.cpp` - функциоанльные тесты
- `tests/performance/main.cpp` - тесты производительности

**Ключевые классы:**
- `BatushinIMaxValRowsMatrixSEQ` - последовательная реализация
- `BatushinIMaxValRowsMatrixMPI` - параллельная MPI реализация

**Основные методы:**
- `ValidationImpl()` - проверка входных данных
- `PreProcessingImpl()` - подготовительные вычисления  
- `RunImpl()` - основной алгоритм
- `PostProcessingImpl()` - завершающая обработка

**Допущения:**
- Матрица хранится построчно
- Все процессы имеют доступ ко всей матрице (в MPI версии)
- Размеры матрицы корректны (M > 0, N > 0)

**Обрабатываемые граничные случаи:**
- Матрица 1×1 (одна строка, один столбец)
- Матрица 1×N (одна строка, несколько столбцов)
- Матрица M×1 (несколько строк, один столбец)
- Все элементы в строке одинаковые
- Отрицательные числа в матрице
- Неравномерное распределение строк при MPI распараллеливании

**Проверки в ValidationImpl():**
```cpp
if (rows == 0 || columns == 0) return false;           // нулевые размеры
if (matrix.size() != rows * columns) return false;     // несоответствие размеров
if (!GetOutput().empty()) return false;               // выход уже заполнен
```

## 6. Экспериментальное окружение

### 6.1 Аппаратное обеспечение/ОС:

- **Процессор:** Intel Core i5-1135G7
- **Ядра:** 4 физических ядра (8 логических потоков)  
- **ОЗУ:** 8 ГБ DDR4
- **ОС:** WSL Ubuntu 24.04.3 LTS (Linux kernel 5.15)

### 6.2 Программный инструментарий

- **Компилятор:** g++ 13.3.0
- **Тип сборки:** Release
- **Стандарт C++:** C++20
- **MPI:** OpenMPI 4.1.6

### 6.3 Тестовое окружение

```bash
PPC_NUM_PROC=1,2,4,8
```

## 7. Результаты

### 7.1. Корректность работы

Все функциональные тесты пройдены успешно:
- Матрицы различных размеров (от 1×1 до больших размеров)
- Случаи с одним элементом в строке
- Матрицы с отрицательными числами
- Матрицы с одинаковыми значениями во всех строках
- Неравномерное распределение строк между процессами
- SEQ и MPI версии выдают идентичные результаты для всех тестовых случаев

### 7.2. Производительность

**Время выполнения (секунды) для матрицы 5000×5000:**

| Версия | Количество процессов | Task Run время |
|--------|---------------------|----------------|
| SEQ    | 1                   | 0.0272         |
| MPI    | 1                   | 0.0291         |
| MPI    | 2                   | 0.0142         |
| MPI    | 4                   | 0.0114         |

**Ускорение относительно SEQ версии:**

| Количество процессов | Ускорение | Эффективность |
|---------------------|-----------|---------------|
| 1                   | 0.93×     | 93%           |
| 2                   | 1.92×     | 96%           |
| 4                   | 2.39×     | 60%           |


**Формула ускорения:** Ускорение = Время SEQ / Время MPI

**Формула эффективности:** Эффективность = (Ускорение / Количество процессов) × 100%

### 7.3. Анализ эффективности

- **Лучшее ускорение:** 2.39× на 4 процессах (Task Run время)
- **Оптимальная конфигурация:** 2-4 процесса
- **Эффективность MPI:** Высокая при 2 процессах (96%), снижается при увеличении числа процессов

### 7.4. Наблюдения

1. **MPI с 1 процессом** показывает схожую производительность с SEQ версией
2. **MPI с 2 процессами** демонстрирует почти линейное ускорение
3. **MPI с 4 процессами** достигает максимального ускорения для данной задачи

## 8. Выводы

### 8.1. Достигнутые результаты

- **Корректность:** Разработанный алгоритм успешно прошел все функциональные тесты, включая граничные случаи
- **Эффективность параллелизации:** Достигнуто ускорение до 2.39× на 4 процессах для матрицы 5000×5000
- **Оптимальная конфигурация:** Наилучшая эффективность (96%) достигнута при использовании 2 процессов

### 8.2. Ограничения и проблемы

- **Накладные расходы MPI:** При использовании 4 процессов эффективность снижается до 60% из-за коммуникационных затрат
- **Ограничения аппаратуры:** Максимальное ускорение ограничено 4 физическими ядрами процессора
- **Размер данных:** Для матриц меньшего размера накладные расходы MPI могут превышать выгоду от параллелизации

## 9. Источники
1. Лекции по параллельному программированию Сысоева А. В
2. Материалы курса: https://github.com/learning-process/ppc-2025-processes-engineers

## 10. Приложение

```cpp
bool BatushinIMaxValRowsMatrixMPI::RunImpl() {
  int rank = 0;
  int proc = 0;

  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &proc);

  size_t rows = std::get<0>(GetInput());
  size_t columns = std::get<1>(GetInput());
  const auto &matrix = std::get<2>(GetInput());

  size_t base_rows = rows / proc;
  size_t extra_rows = rows % proc;

  size_t start_row = (rank * base_rows) + std::min<size_t>(rank, extra_rows);
  size_t end_row = start_row + base_rows + (std::cmp_less(rank, extra_rows) ? 1 : 0);

  std::vector<double> loc_max;

  for (size_t i = start_row; i < end_row; i++) {
    double max_val = matrix[i * columns];
    for (size_t j = 1; j < columns; j++) {
      double val = matrix[(i * columns) + j];
      max_val = std::max(val, max_val);
    }
    loc_max.push_back(max_val);
  }

  std::vector<double> res;
  if (rank == 0) {
    res.resize(rows);

    for (size_t i = 0; i < loc_max.size(); i++) {
      res[start_row + i] = loc_max[i];
    }

    for (int src = 1; src < proc; src++) {
      size_t src_start = (src * base_rows) + std::min<size_t>(src, extra_rows);
      size_t src_size = base_rows + (std::cmp_less(src, extra_rows) ? 1 : 0);

      std::vector<double> recv_buf(src_size);
      MPI_Recv(recv_buf.data(), static_cast<int>(src_size), MPI_DOUBLE, src, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

      for (size_t i = 0; i < src_size; i++) {
        res[src_start + i] = recv_buf[i];
      }
    }
  } else {
    MPI_Send(loc_max.data(), static_cast<int>(loc_max.size()), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);
  }

  if (rank == 0) {
    int res_size = static_cast<int>(res.size());
    MPI_Bcast(&res_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

    MPI_Bcast(res.data(), res_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  } else {
    int res_size = 0;
    MPI_Bcast(&res_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

    res.resize(res_size);
    MPI_Bcast(res.data(), res_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  }

  GetOutput() = res;

  return true;
}
```