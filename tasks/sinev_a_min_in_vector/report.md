# Минимальное значение элементов вектора

- Студент: Синев Артём Александрович, группа 3823Б1ПР2
- Технологии: SEQ, MPI
- Вариант: 4

## 1. Введение

Задача поиска минимального элемента в векторе является одной из фундаментальных операций в вычислительной математике и анализе данных. Данная работа посвящена разработке и реализации параллельного алгоритма для поиска минимального элемента в векторе с использованием технологии MPI (Message Passing Interface). Алгоритм обеспечивает эффективное распределение вычислений между процессами и позволяет масштабировать решение на большое количество вычислительных узлов.

## 2. Постановка задачи

Дан вектор целых чисел. Требуется найти минимальный элемент.

**Входные данные:**
- vector - одномерный вектор целых чисел произвольной длины N

**Выходные данные:**
- min_value - целое число, содержащее минимальное значение в векторе

**Ограничения:**
- Вектор должен содержать хотя бы один элемент
- Элементы вектора - целые числа (int)
- Допускаются отрицательные значения

**Пример:**

Входные данные: vector = [5, 3, 8, 1, 9, 2]
Выходные данные: min_value = 1

## 3. Описание базового алгоритма (последовательная версия)

Последовательный алгоритм поиска минимального элемента в векторе выполняет линейный проход по всем элементам с обновлением текущего минимального значения.

Алгоритм состоит из следующих шагов:

1. **Инициализация**: Установка начального минимального значения как первого элемента вектора
2. **Обработка строк**: Для каждого элемента от 1 до N-1::
   - Сравнение текущего элемента с сохраненным минимальным значением
   - Обновление минимального значения при обнаружении меньшего элемента
3. **Завершение**: Возврат найденного минимального значения

**Сложность алгоритма**

**Время:** O(N)
- Один проход по всем элементам вектора
- N-1 операций сравнения

**Память:** O(N)
- O(N) для хранения входного вектора
- O(1) для временных переменных
- **Итого:** O(N)

## 4. Схема распараллеливания

Параллельная реализация основана на разделении вектора на блоки между доступными процессами с использованием стратегии блочного распределения

```cpp
block_size = vector_size / proc_count      // Базовый размер блока
remainder = vector_size % proc_count       // Остаточные элементы
```

**Распределение работы**

- Каждый процесс получает непрерывный блок элементов
- Процессы с меньшим rank получают дополнительный элемент при наличии остатка
- Каждый процесс находит локальный минимум в своем блоке
- Глобальный минимум вычисляется с помощью операции MPI_Allreduce

**Роли процессов**
- **Все процессы:** Выполняют вычисления на выделенных порциях данных
- **MPI_Allreduce:** Автоматически синхронизирует и вычисляет глобальный минимум

**Схема коммуникации**
- **Фаза вычислений:** Каждый процесс независимо вычисляет локальный минимум
- **Фаза сбора:** MPI_Allreduce собирает все локальные минимумы и находит глобальный минимум
- **Фаза синхронизации:** Все процессы получают одинаковый глобальный результат

## 5. Детали реализации

**Файлы:**
- `common/include/common.hpp` - определение типов данных
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` - последовательная реализация
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` - параллельная реализация
- `tests/functional/main.cpp` - функциоанльные тесты
- `tests/performance/main.cpp` - тесты производительности

**Ключевые классы:**
- `BatushinIMaxValRowsMatrixSEQ` - последовательная реализация
- `BatushinIMaxValRowsMatrixMPI` - параллельная MPI реализация

**Основные методы:**
- `ValidationImpl()` - проверка входных данных
- `PreProcessingImpl()` - подготовительные вычисления  
- `RunImpl()` - основной алгоритм
- `PostProcessingImpl()` - завершающая обработка

**Допущения:**
- Вектор доступен всем процессам (реплицированные данные)
- Размер вектора достаточен для эффективного распараллеливания
- Все процессы участвуют в вычислениях

**Обрабатываемые граничные случаи:**
- Вектор из одного элемента
- Вектор с отрицательными числами
- Вектор, где все элементы одинаковые
- Вектор, где минимальный элемент находится в разных позициях
- Неравномерное распределение элементов при MPI распараллеливании

**Проверки в ValidationImpl():**
```cpp
bool SinevAMinInVectorMPI::RunImpl() {
  int proc_num = 0;
  int proc_rank = 0;

  MPI_Comm_size(MPI_COMM_WORLD, &proc_num);
  MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);

  std::vector<int> &my_vector = GetInput();
  int min_number = INT_MAX;

  // Распределение данных между процессами
  int block_vector = static_cast<int>(my_vector.size()) / proc_num;
  int remainder = static_cast<int>(my_vector.size()) % proc_num;

  int start_index = (proc_rank * block_vector) + std::min(proc_rank, remainder);
  int end_index = start_index + block_vector + (proc_rank < remainder ? 1 : 0);

  // Локальное вычисление минимума
  for (int i = start_index; i < end_index; i++) {
    min_number = std::min(my_vector[i], min_number);
  }

  // Глобальная редукция для нахождения общего минимума
  int general_min = INT_MAX;
  MPI_Allreduce(&min_number, &general_min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);

  GetOutput() = general_min;
  return true;
}
```

## 6. Экспериментальное окружение

### 6.1 Аппаратное обеспечение/ОС:

- **Процессор:** Intel Core i7-13700HX
- **Ядра:** 16 физических ядер  
- **ОЗУ:** 8 ГБ 
- **ОС:** Kubuntu 24.04

### 6.2 Программный инструментарий

- **Компилятор:** g++ 13.3.0
- **Тип сборки:** Release
- **Стандарт C++:** C++20
- **MPI:** OpenMPI 4.1.6

### 6.3 Тестовое окружение

```bash
PPC_NUM_PROC=1,2,4
```

## 7. Результаты

### 7.1. Корректность работы

Все функциональные тесты пройдены успешно:
- mixed_positive: Вектор со смешанными положительными числами [5, 3, 8, 1, 9, 2]
- with_negatives: Вектор с отрицательными числами [10, -5, 7, 0, 15] 
- single_element: Вектор из одного элемента [42] 
- zero_only: Вектор с нулевым элементом [0]
- all_negative: Вектор только с отрицательными числами [-10, -140, -45, -24, -99]

SEQ и MPI версии выдают идентичные результаты для всех тестовых случаев.

### 7.2. Производительность

**Время выполнения (секунды) для матрицы 5000×5000:**

| Версия | Количество процессов | Task Run время | Pipeline время |
|--------|---------------------|----------------|-----------------|
| SEQ    | 1                   | 0.0428         | 0.0402          |
| MPI    | 1                   | 0.0441         | 0.0415          |
| MPI    | 2                   | 0.0232         | 0.0218          |
| MPI    | 4                   | 0.0124         | 0.0116          |

**Ускорение относительно SEQ версии:**

| Количество процессов | Ускорение | Эффективность |
|---------------------|-----------|---------------|
| 1                   | 0.97×     | 97%           |
| 2                   | 1.84×     | 92%           |
| 4                   | 3.45×     | 86%           |


**Формула ускорения:** Ускорение = Время SEQ / Время MPI

**Формула эффективности:** Эффективность = (Ускорение / Количество процессов) × 100%

### 7.3. Анализ эффективности

- **Лучшее ускорение:** 3.45× на 4 процессах (Task Run время)
- **Оптимальная конфигурация:** 4 процесса
- **Эффективность MPI:** Высокая при 2 процессах (92%), хорошая при 4 процессах (86%)

### 7.4. Наблюдения

1. **MPI с 1 процессом** показывает схожую производительность с SEQ версией (небольшие накладные расходы)
2. **MPI с 2 процессами** демонстрирует почти линейное ускорение
3. **MPI с 4 процессами** достигает максимального ускорения для данной задачи
4. **Pipeline время** немного лучше чем Task Run время для всех конфигураций

## 8. Выводы

### 8.1. Достигнутые результаты

- **Корректность:** Разработанный алгоритм успешно прошел все функциональные тесты, включая граничные случаи
- **Эффективность параллелизации:** Достигнуто ускорение до 3.45× на 4 процессах для вектора 50 миллионов элементов
- **Масштабируемость:** Алгоритм эффективно масштабируется с увеличением количества процессов
- **Оптимальная конфигурация:** Наилучшая эффективность достигается при использовании 4 процессов

### 8.2. Ограничения и проблемы

- **Накладные расходы MPI:** При использовании 1 процесса наблюдается небольшое замедление из-за коммуникационных затрат
- **Ограничения аппаратуры:** Максимальное ускорение ограничено количеством физических ядер процессора
- **Размер данных:** Для векторов меньшего размера накладные расходы MPI могут превышать выгоду от параллелизации

### 8.3. Преимущества подхода

- **Простота реализации:** Использование MPI_Allreduce значительно упрощает код и обеспечивает автоматическую синхронизацию
- **Масштабируемость:** Алгоритм легко адаптируется для разного количества процессов
- **Универсальность:** Подход применим для различных типов данных и операций редукции
- **Баланс нагрузки:** Стратегия блочного распределения обеспечивает равномерную загрузку процессов

## 9. Источники
1. Лекции по параллельному программированию Сысоева А. В
2. Документация MPI: https://www.open-mpi.org/
3. Материалы курса: https://github.com/learning-process/ppc-2025-processes-engineers

## 10. Приложение

```cpp
bool SinevAMinInVectorMPI::RunImpl() {
  int proc_num = 0;
  int proc_rank = 0;

  MPI_Comm_size(MPI_COMM_WORLD, &proc_num);
  MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);

  std::vector<int> &my_vector = GetInput();
  int min_number = INT_MAX;

  // Распределение данных между процессами
  int block_vector = static_cast<int>(my_vector.size()) / proc_num;
  int remainder = static_cast<int>(my_vector.size()) % proc_num;

  int start_index = (proc_rank * block_vector) + std::min(proc_rank, remainder);
  int end_index = start_index + block_vector + (proc_rank < remainder ? 1 : 0);

  // Локальное вычисление минимума
  for (int i = start_index; i < end_index; i++) {
    min_number = std::min(my_vector[i], min_number);
  }

  // Глобальная редукция для нахождения общего минимума
  int general_min = INT_MAX;
  MPI_Allreduce(&min_number, &general_min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);

  GetOutput() = general_min;
  return true;
}
```